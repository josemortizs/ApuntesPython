#+TITLE: Scikit
#+AUTHOR: David Arroyo Menéndez
#+OPTIONS: H:2 toc:nil num:t
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+BEAMER_THEME: Madrid
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)

* Summary
** Scikit
#+BEGIN_SRC bash
$ git clone https://github.com/davidam/python-examples.git
#+END_SRC

1. Classification
2. Clustering
3. Regression
4. Dimensionality Reduction

**** This will be formatted as a beamer note                       :B_note:
     :PROPERTIES:
     :BEAMER_env: note
     :END:

** Scikit Graph

file:img/scikit-graph.png

** Datasets

Any machine learning exercise requires datasets. You can load datasets
with scikit. If you are creating a new dataset you can use get_data_home:

#+BEGIN_SRC python
from sklearn.datasets import get_data_home
        data_path = os.path.join(get_data_home(), "reuters")
#+END_SRC

You can create automated datasets, or use sklearn datasets.

#+BEGIN_SRC bash
$ python3 plot_random_dataset.py
$ python3 rcv1.py
$ python3 lfw.py
#+END_SRC

** Classification

file:img/classification.png

** Classification: SGD

We want predict a category and we’ve labeled
data with <100k samples
#+BEGIN_SRC bash
$ python3 plot_sgd_iris.py
$ python3 sgd.py
#+END_SRC

** Classification: Kernel Approximation
We want predict a category and we’ve labeled
data with <100k samples and SGD is not working

#+BEGIN_SRC bash
$ python3 kernel-approximation.py
#+END_SRC

** Classification: Naive Bayes

file:img/naivebayes.png

** Classification: Naive Bayes

file:img/naivebayes-table.png

** Classification: Naive Bayes

file:img/naivebayes-table2.png

** Classification: Naive Bayes Examples

#+BEGIN_SRC bash
$ python3 gaussiannb.py
$ python3 bernoullinb.py
$ python3 multinomialnb.py
$ python3 nltk/sexmachine.py
#+END_SRC

** Classification: SVC

We want predict a category and we’ve labeled
data with <100k samples

file:img/svc-tables.png

** Classification: SVC. What's a kernel?

file:img/kernel-functions.png

** Classification. SVC

We want predict a category and we’ve labeled
data with <100k samples

#+BEGIN_SRC bash
$ python3 svc-example.py
#+END_SRC

** Classification: Trees

ID3: The algorithm creates a multiway tree, finding for each node (i.e. in a
greedy manner) the categorical feature that will yield the largest information gain
for categorical targets. Trees are grown to their maximum size and then a
pruning step is usually applied to improve the ability of the tree to generalise to
unseen data.

C4.5: is the successor to ID3 and removed the restriction that features must
be categorical by dynamically defining a discrete attribute (based on numerical
variables) that partitions the continuous attribute value into a discrete set of
intervals.

CART : is very similar to C4.5, but it differs in that it supports numerical target
variables (regression) and does not compute rule sets.

** Classification: Trees (II)

#+BEGIN_SRC bash
$ python3 plot_iris1.py
$ python3 plot_unveil_tree_structure.py
$ python3 plot_classifier_comparison.py
$ python3 plot_ensemble_oob.py
$ python3 datasets/decisiontree.py
#+END_SRC

** Regression

file:img/regression.png

** SGD Regressor

The class SGDClassifier implements a plain stochastic
gradient descent learning routine which supports different
loss functions and penalties for classification.

file:img/sgd-regressor.png

** SGD Regressor (II)

As other classifiers, SGD has to be fitted with two
arrays: an array X of size [n_samples, n_features]
holding the training samples, and an array Y of size
[n_samples] holding the target values (class labels) for
the training samples:

#+BEGIN_SRC bash
$ python3 plot_sgd_iris.py
$ python3 sgd.py
#+END_SRC

** Lasso

Linear Model trained with L1 prior as regularizer (aka the
Lasso)
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Technically the Lasso model is optimizing the same objective
function as the Elastic Net with l1_ratio=1.0 (no L2 penalty).

#+BEGIN_SRC bash
$ python3 plot_lasso_and_elasticnet.py
$ python3 plot_multi_task_lasso_support.py
#+END_SRC

** Kernel Ridge

Kernel ridge regression (KRR) [M2012] combines Ridge
Regression (linear least squares with l2-norm regularization)
with the kernel trick. It thus learns a linear function in the
space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function
in the original space.

#+BEGIN_SRC bash
$ python3 plot_kernel_ridge_regression.py
#+END_SRC

** Support Vector Regression

The free parameters in the model are C and epsilon.

#+BEGIN_SRC bash
$ python3 svr-example.py
#+END_SRC

file:img/svr.png

** Clustering

file:img/clustering.png

** Kmeans

file:img/kmeans.png

** Kmeans

Subdivide the space making regions from reference points called
centroides

#+BEGIN_SRC bash
$ python3 plot_kmeans_assumptions.py
$ python3 plot_cluster_iris.py
#+END_SRC

** GMM

A typical finite-dimensional mixture model is a hierarchical model consisting of the following components:

N random variables that are observed, each distributed according to a mixture of K components, with the components belonging to the same parametric family of distributions (e.g., all normal, all Zipfian, etc.) but with different parameters
N random latent variables specifying the identity of the mixture component of each observation, each distributed according to a K-dimensional categorical distribution
A set of K mixture weights, which are probabilities that sum to 1.
A set of K parameters, each specifying the parameter of the corresponding mixture component. In many cases, each "parameter" is actually a set of parameters. For example, if the mixture components are Gaussian distributions, there will be a mean and variance for each component. If the mixture components are categorical distributions (e.g., when each observation is a token from a finite alphabet of size V), there will be a vector of V probabilities summing to 1.

#+BEGIN_SRC bash
$ python3 plot_gmm_covariances.py
#+END_SRC

** Spectral Clustering

Make use of the spectrum (eigenvalues) of the similarity matrix of the
data to perform dimensionality reduction before clustering in fewer
dimensions

#+BEGIN_SRC bash
$ python3 plot_cluster_comparison.py
#+END_SRC

** Mean Shift
Mean shift is a non-parametric feature-space analysis technique for
locating the maxima of a density function, a so-called mode-seeking
algorithm.

#+BEGIN_SRC bash
$ python3 plot_mean_shift.py
#+END_SRC

** Dimensionality Reduction

file:img/dimensionality-reduction.png

The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets

** Randomized PCA

PCA is mostly used as a tool in exploratory data analysis and for
making predictive models. It's often used to visualize genetic
distance and relatedness between populations. PCA can be done by
eigenvalue decomposition of a data covariance (or correlation) matrix
or singular value decomposition of a data matrix, usually after mean
centering

#+BEGIN_SRC bash
$ python3 pca-example.py
$ python3 pca.py
$ python3 scikit-plot-pca.py
$ python3 plot_pca_iris.py
$ python3 incremental-pca.py
#+END_SRC

** Kernel Approximation

The general task of pattern analysis is to find and study general
types of relations (for example clusters, rankings, principal
components, correlations, classifications) in datasets.

For many algorithms that solve these tasks, the data in raw
representation have to be explicitly transformed into feature vector
representations via a user-specified feature map: in contrast, kernel
methods require only a user-specified kernel, i.e., a similarity
function over pairs of data points in raw representation.

#+BEGIN_SRC bash
$ python3 kernel-approximation.py
#+END_SRC

** LLE and Spectral Embedding

#+BEGIN_SRC bash
$ python3 plot_lle_digits.py
$ python3 plot_spectral_grid.py
#+END_SRC

Spectral embedding for non-linear dimensionality reduction.

Forms an affinity matrix given by the specified function and applies
spectral decomposition to the corresponding graph laplacian. The
resulting transformation is given by the value of the eigenvectors for
each data point.

** Isomap

Isomap Embedding
Non-linear dimensionality reduction through Isometric Mapping

#+BEGIN_SRC bash
$ python3 plot_compare_methods.py
#+END_SRC

** Scikit Image

#+BEGIN_SRC bash
$ python3 plot_marching_cubes.py
#+END_SRC

** Conceptos Clave

file:img/key-concepts.png

** Confusion Matrix

Each row of the matrix represents the instances in a predicted class
while each column represents the instances in an actual class.

#+BEGIN_SRC bash
$ python3 plot_confusion_matrix.py
#+END_SRC

In a confusion matrix you must understand to make an accuracy between
the predicted class and the real class.

#+BEGIN_SRC bash
$ python3 accuracy.py
#+END_SRC

This exercise is useful to understand the details to build a confusion matrix

#+BEGIN_SRC bash
$ python3 confusion-matrix.py
#+END_SRC

** Leave One Out

#+BEGIN_SRC bash
$ python3 leaveoneout.py
$ python3 leavepout.py
#+END_SRC

** Validación Cruzada

#+BEGIN_SRC bash
$ python3 crossvalidation.py
$ python3 repeatedkfold.py
#+END_SRC

** Overfitting

Avoiding troubles with additional data

#+BEGIN_SRC bash
$ python3 nooverfitting.py
#+END_SRC

** Outliers

Son valores atípicos que se salen del doble de la desviación típica,
por ejemplo.

#+BEGIN_SRC bash
$ python3 numpy/reject-outliers.py
$ python3 scikit/plot_outlier_detection_housing.py
#+END_SRC

** Pipelines
Es posible hacer pipes dese un algoritmo a otro usando esta librería

#+BEGIN_SRC bash
$ python3 sckikit/pipeline.py
#+END_SRC

** Comparing Classifiers

#+BEGIN_SRC bash
$ python3 plot_classifier_commparison.py
#+END_SRC

Dados puntos azules y rojos aprende el espacio donde se sitúan estos.

#+BEGIN_SRC bash
$ python3 plot_compare_methods.py
#+END_SRC

En reducción de dimensiones, podemos imaginar una imagen
tridimensional de bolas aplanarla a una imagen bidimensional.

#+BEGIN_SRC bash
$ python3 plot_compare_calibration.py
$ python3 plot_calibration_curve.py
#+END_SRC

Otra manera interesante de comparar reducción de dimensiones es con
grid search y pipeline

#+BEGIN_SRC bash
$ python3 plot_compare_reduction.py
#+END_SRC


Otra cuestión son los clasificadores bien calibrados (probabilísticos).

Para aprender y avanzar: http://benalexkeen.com/scoring-classifier-models-using-scikit-learn/

** Model selection and evaluation (I)
*** Cross-validation: evaluating estimator performance
+ Computing cross-validated metrics: scores (Ej: ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro'])
+ Cross validation iterators (K-fold, Repeated K-Fold, Leave One Out (LOO), Leave P Out (LPO), ...)
*** Tuning the hyper-parameters of an estimator
+ Exhaustive Grid Search
+ Randomized Parameter Optimization
+ Tips for parameter search
** Model selection and evaluation (II)
*** Model evaluation: quantifying the quality of predictions
+ The scoring parameter: defining model evaluation rules
+ Classification metrics
+ Multilabel ranking metrics
+ Regression metrics
*** Model persistence
Example: pickle, joblib, ...


** Building Machine Learning from scratch

Uno puede construir sus propios algoritmos de machine learning,
leyendo desde tutoriales, o artículos científicos

#+BEGIN_SRC bash
$ python3 text-classification.py
$ python3 layerneuralnetwork.py
#+END_SRC
